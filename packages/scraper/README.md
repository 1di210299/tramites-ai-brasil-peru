# ğŸ•·ï¸ Scraper TUPA - Sistema de ExtracciÃ³n de Datos

Sistema completo de scraping para extraer y estructurar datos de procedimientos administrativos (TUPA) del gobierno peruano.

## ğŸ¯ CaracterÃ­sticas Principales

- **Scraping AsÃ­ncrono**: Utiliza `aiohttp` y `asyncio` para scraping eficiente
- **MÃºltiples Fuentes**: Extrae datos de gob.pe, SUNAT, RENIEC, SUNARP y otras entidades
- **DetecciÃ³n Anti-Bot**: Usa `undetected-chromedriver` y rotaciÃ³n de User-Agents
- **IntegraciÃ³n BD**: ConexiÃ³n directa con PostgreSQL usando SQLAlchemy async
- **EstructuraciÃ³n Inteligente**: Clasifica automÃ¡ticamente procedimientos por categorÃ­a
- **ExportaciÃ³n MÃºltiple**: Genera JSON, CSV y formatos optimizados para frontend

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerequisitos

1. **Python 3.9+**
2. **Chrome/Chromium** (para Selenium)
3. **PostgreSQL** (para almacenamiento)

### InstalaciÃ³n

```bash
# Navegar al directorio del scraper
cd packages/scraper

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno (opcional)
cp .env.example .env
```

### Variables de Entorno

```bash
# Database (opcional - usa defaults si no se especifica)
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/tramites_db

# ConfiguraciÃ³n de scraping
SCRAPER_DELAY=2  # Delay entre requests en segundos
SCRAPER_MAX_PAGES=50  # MÃ¡ximo de pÃ¡ginas a procesar
SCRAPER_TIMEOUT=30  # Timeout para requests HTTP
```

## ğŸ“‹ Uso del Sistema

### Scraping Completo

```bash
# Ejecutar scraping completo (recomendado)
python src/scraping_orchestrator.py --mode full

# Solo scraping sin guardar en BD
python src/scraping_orchestrator.py --mode full --no-db

# Solo scraping sin exportar archivos
python src/scraping_orchestrator.py --mode full --no-export
```

### Scraping Incremental

```bash
# ActualizaciÃ³n incremental (solo nuevos procedimientos)
python src/scraping_orchestrator.py --mode incremental
```

### ValidaciÃ³n de Datos

```bash
# Validar integridad de datos en BD
python src/scraping_orchestrator.py --mode validate
```

### Uso ProgramÃ¡tico

```python
from src.scraping_orchestrator import ScrapingOrchestrator

# Crear orchestrador
orchestrator = ScrapingOrchestrator()

# Ejecutar proceso completo
results = await orchestrator.run_full_process()

print(f"Procedimientos scrapeados: {results['procedures_scraped']}")
print(f"Guardados en BD: {results['procedures_saved']}")
```

## ğŸ—ï¸ Arquitectura del Sistema

### Componentes Principales

1. **`tupa_scraper.py`**: Scraper principal con lÃ³gica de extracciÃ³n
2. **`database_integration.py`**: IntegraciÃ³n con PostgreSQL
3. **`scraping_orchestrator.py`**: Orchestrador que coordina todo el proceso

### Flujo de Trabajo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraping      â”‚â”€â”€â”€â–¶â”‚   EstructuraciÃ³n â”‚â”€â”€â”€â–¶â”‚  Almacenamiento â”‚
â”‚   Web Sites     â”‚    â”‚   y ValidaciÃ³n   â”‚    â”‚   PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ gob.pe, SUNAT   â”‚    â”‚ ClasificaciÃ³n    â”‚    â”‚ ExportaciÃ³n     â”‚
â”‚ RENIEC, SUNARP  â”‚    â”‚ CategorizaciÃ³n   â”‚    â”‚ JSON, CSV       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Datos ExtraÃ­dos

Para cada procedimiento se extrae:

- **InformaciÃ³n BÃ¡sica**: Nombre, descripciÃ³n, entidad responsable
- **Detalles TÃ©cnicos**: CÃ³digo TUPA, requisitos, costo, tiempo de procesamiento
- **ClasificaciÃ³n**: CategorÃ­a, dificultad, disponibilidad online
- **Metadatos**: URL fuente, palabras clave, base legal

### Entidades Cubiertas

- **SUNAT**: TrÃ¡mites tributarios, RUC, declaraciones
- **RENIEC**: DNI, pasaportes, documentos de identidad
- **SUNARP**: Registros pÃºblicos, empresas, propiedades
- **MINSA**: TrÃ¡mites de salud, certificados mÃ©dicos
- **Municipalidades**: Licencias, funcionamiento, construcciÃ³n

### CategorÃ­as de Procedimientos

- `identidad`: Documentos de identificaciÃ³n
- `empresarial`: ConstituciÃ³n y gestiÃ³n de empresas
- `tributario`: Impuestos y obligaciones fiscales
- `vehicular`: Licencias de conducir, vehÃ­culos
- `educacion`: TÃ­tulos, certificados educativos
- `salud`: TrÃ¡mites mÃ©dicos y sanitarios
- `laboral`: Empleo y relaciones laborales
- `municipal`: TrÃ¡mites municipales y locales

## ğŸ“ Archivos Generados

### JSON Completo
```json
{
  "name": "Duplicado de DNI por Deterioro",
  "description": "ObtenciÃ³n de un nuevo DNI cuando el documento se encuentra deteriorado",
  "entity_name": "RENIEC",
  "entity_code": "RENIEC",
  "tupa_code": "RENIEC-001",
  "requirements": ["DNI deteriorado original", "Recibo de pago"],
  "cost": 32.20,
  "currency": "PEN",
  "processing_time": "48 horas",
  "category": "identidad",
  "is_free": false,
  "is_online": false,
  "difficulty_level": "easy"
}
```

### JSON para Frontend
```json
{
  "metadata": {
    "total_procedures": 150,
    "last_updated": "2024-01-15T10:30:00",
    "entities": ["SUNAT", "RENIEC", "SUNARP"],
    "categories": ["identidad", "empresarial", "tributario"]
  },
  "procedures": [...]
}
```

### CSV para AnÃ¡lisis
```csv
Nombre,Entidad,Categoria,Costo,Es_Gratuito,Es_Online,Dificultad
Duplicado de DNI,RENIEC,identidad,32.20,False,False,easy
InscripciÃ³n RUC,SUNAT,tributario,0.00,True,True,easy
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Personalizar Scraping

```python
# En tupa_scraper.py, modificar configuraciÃ³n
class TupaScraper:
    def __init__(self):
        self.base_urls = {
            'custom_entity': 'https://custom-entity.gob.pe'
        }
        
        # Agregar nueva categorÃ­a
        self.categories['nueva_categoria'] = ['keyword1', 'keyword2']
```

### Extender Entidades

```python
async def _scrape_custom_entity(self) -> List[ProcedureData]:
    """Scraper especÃ­fico para entidad personalizada"""
    procedures = []
    
    # Implementar lÃ³gica especÃ­fica
    custom_data = [
        {
            'name': 'Procedimiento Custom',
            'description': 'DescripciÃ³n del procedimiento',
            # ... mÃ¡s campos
        }
    ]
    
    for data in custom_data:
        procedure = ProcedureData(**data)
        procedures.append(procedure)
    
    return procedures
```

## ğŸ“ˆ Monitoreo y Logging

### Logs Generados

- `scraping.log`: Log detallado del proceso
- `scraping_report.txt`: Reporte final con estadÃ­sticas
- Salida en consola con progreso en tiempo real

### MÃ©tricas Disponibles

```python
results = {
    'procedures_scraped': 150,
    'procedures_saved': 145,
    'errors': 5,
    'entities_processed': {'SUNAT', 'RENIEC', 'SUNARP'},
    'duration_seconds': 120.5
}
```

## ğŸ› Troubleshooting

### Errores Comunes

1. **Chrome Driver Error**
   ```bash
   # Instalar Chrome/Chromium
   # Ubuntu/Debian
   sudo apt-get install chromium-browser
   
   # macOS
   brew install --cask google-chrome
   ```

2. **Database Connection Error**
   ```bash
   # Verificar PostgreSQL estÃ¡ corriendo
   # Verificar credenciales en DATABASE_URL
   # Crear base de datos si no existe
   ```

3. **Rate Limiting/Blocking**
   ```python
   # Aumentar delays en configuraciÃ³n
   SCRAPER_DELAY=5  # Aumentar delay entre requests
   ```

### Debug Mode

```bash
# Ejecutar con logging detallado
python -u src/scraping_orchestrator.py --mode full 2>&1 | tee debug.log
```

## ğŸ¤ Contribuir

1. Fork del repositorio
2. Crear rama para feature: `git checkout -b feature/nueva-entidad`
3. Implementar cambios
4. Agregar tests si es necesible
5. Commit: `git commit -am 'Add nueva entidad scraper'`
6. Push: `git push origin feature/nueva-entidad`
7. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Para soporte y preguntas:
- Crear issue en GitHub
- Revisar logs en `scraping.log`
- Verificar configuraciÃ³n de base de datos

---

## ğŸ¯ PrÃ³ximas Funcionalidades

- [ ] Scraping inteligente con ML para detectar nuevos patrones
- [ ] API REST para ejecutar scraping bajo demanda
- [ ] Dashboard web para monitoreo en tiempo real
- [ ] Notificaciones automÃ¡ticas de nuevos procedimientos
- [ ] IntegraciÃ³n con sistemas de cachÃ© (Redis)
- [ ] Scraping diferencial (solo cambios)
- [ ] Soporte para mÃ¡s formatos de exportaciÃ³n (Excel, XML)
- [ ] AnÃ¡lisis de sentimientos en descripciones de procedimientos
